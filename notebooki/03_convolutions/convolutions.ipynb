{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How to train your own model",
   "id": "12b793e1e258c3df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zacznijmyd od podstawowych importów i sprawdzenia jakie mamy możliwe środki do trenowania",
   "id": "4bf6b248ad460867"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "Dzisiaj będziemy pracować z bardziej skomplikowanym przykładem jakim jest [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). Jest to zbiór obrazów o rozmiarze 32x32 pikseli, zawierający 10 klas obiektów. Możemy go pobrać z biblioteki torchvision."
   ],
   "id": "27a26317071bd11c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")"
   ],
   "id": "987f9f864e684de0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# teraz chcemy ustawić wielkość batcha i learning rate do trenowania\n",
    "\n",
    "# TODO: spróbuj pozmieniać wartości tych parametrów\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# i przygotować dataset do załadowania\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")"
   ],
   "id": "3e46d0a112a207eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Wizualizacja danych\n",
    "Zobaczmy jak wyglądają nasze dane. W tym celu użyjemy biblioteki matplotlib, aby wyświetlić kilka obrazów wraz z podpisami z naszego zbioru danych."
   ],
   "id": "be29383321fb33aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_images(images: torch.Tensor, labels: torch.Tensor, classes: list[str]) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0))  # permute to change from CxHxW to HxWxC\n",
    "        plt.title(classes[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ],
   "id": "8647c983ce29e2ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# zobaczmy pierwsze 10 obrazków z naszego datasetu\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "show_images(images[:8], labels[:8], train_dataloader.dataset.classes)"
   ],
   "id": "fc04d679844aaef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(SimpleModel, self).__init__()\n",
    "        ### TODO: zdefiniuj prostą sieć neuronową z 2 ukrytymi warstwami o rozmiarach 128 i 64\n",
    "        self.layer1 = torch.nn.Linear(32 * 32 * 3, 128)\n",
    "        self.layer2 = torch.nn.Linear(128, 64)\n",
    "        self.layer3 = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### TODO: zaimplementuj funkcję forward z użyciem aktywacji ReLU pomiędzy warstwami, pamiętaj o spłaszczeniu wejścia\n",
    "        x = x.flatten(start_dim=1)  # flatten: (batch, 3, 32, 32) -> (batch, 3072)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)  # UWAGA: nie robimy tutaj softmax - CrossEntropyLoss wymaga wartości PRZED softmax\n",
    "        return x"
   ],
   "id": "a6ba00fd0d85f24f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validation_loop(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    ") -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_epochs: int,\n",
    ") -> None:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(train_dataloader)\n",
    "        val_loss, val_acc = validation_loop(model, test_dataloader, criterion)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ],
   "id": "7ec83528c3b54deb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### TODO: trenowanie najmniejszego modelu\n",
    "simple_model = SimpleModel().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(simple_model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_loop(simple_model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)"
   ],
   "id": "a1d9e8723d3a9626",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SequentialModel(torch.nn.Module):\n",
    "    def __init__(self, layers: list[int]) -> None:\n",
    "        super(SequentialModel, self).__init__()\n",
    "\n",
    "        ### TODO: zdefiniuj model sekwencyjny na podstawie podanej listy rozmiarów warstw, użyj aktywacji ReLU pomiędzy warstwami,\n",
    "        ### hint: użyj torch.nn.Sequential oraz torch.nn.Linear\n",
    "        all_layers = []\n",
    "        input_size = 32 * 32 * 3\n",
    "\n",
    "        for output_size in layers:\n",
    "            all_layers.append(torch.nn.Linear(input_size, output_size))\n",
    "            all_layers.append(torch.nn.ReLU())\n",
    "            input_size = output_size\n",
    "\n",
    "        all_layers.append(torch.nn.Linear(input_size, 10))\n",
    "        self.model = torch.nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### TODO: zaimplementuj funkcję forward z użyciem spłaszczenia wejścia\n",
    "        x = x.flatten(start_dim=1)  # flatten: (batch, 3, 32, 32) -> (batch, 3072)\n",
    "        return self.model(x)"
   ],
   "id": "dfe6adb4c0d53bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_bigger = SequentialModel([256, 128, 128, 128, 64]).to(device)\n#### TODO: trenowanie większego modelu\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_bigger.parameters(), lr=learning_rate)\n\ntraining_loop(model_bigger, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)",
   "id": "6e6d3656bf6ec017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Strategia z ostatnich zajęć - konwolucje",
   "id": "d60a765478ed9322"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ConvolutionModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ConvolutionModel, self).__init__()\n",
    "\n",
    "        ### TODO: zdefiniuj model konwolucyjny z 3 warstwami konwolucyjnymi\n",
    "        ### Między konwolucjami użyj aktywacji ReLU oraz MaxPool2d z kernel_size=2\n",
    "        ### Na koniec dodaj warstwę Linear do klasyfikacji na 10 klas\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Po 3 warstwach MaxPool2d: 32 -> 16 -> 8 -> 4\n",
    "        self.fc = torch.nn.Linear(4 * 4 * 128, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### TODO: zaimplementuj funkcję forward\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        return self.fc(x)"
   ],
   "id": "294760dc2260026",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "conv_model = ConvolutionModel().to(device)\n### TODO: trenowanie modelu konwolucyjnego\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(conv_model.parameters(), lr=learning_rate)\n\ntraining_loop(conv_model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)",
   "id": "f36cb97e1c2ae8fc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
